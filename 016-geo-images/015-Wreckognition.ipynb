{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wreckognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will see a few examples of:\n",
    "* geographic tools\n",
    "* using numpy to manipulate images\n",
    "* applying image filters\n",
    "* segmentation\n",
    "* ML-style clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use DAERA's list of protected shipwrecks, hosted by OpenDataNI. For more information, see the [page on their site](https://www.opendatani.gov.uk/dataset/protected-wreck-sites)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these tools, we hope to answer the questions:\n",
    "\n",
    "> Do wrecks cluster near groups of ports?\n",
    "\n",
    "and\n",
    "\n",
    "> Are wrecks more common in deep water or shallow water?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may even try to see whether shipwrecks in the 1800s, 1900s or 2000s have different likely locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will open up the downloaded dataset using geopandas - this is an extended version of pandas that can attach geographic (or geometric) attributes to each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('data/protected_wrecks.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring this data, we see a range of ships - also a few data quality issues in the dates and missing values.\n",
    "\n",
    "If you have a look at the last, \"geometry\", column, you will see an entity in WKT (well-known text) format - a computer-readable way of specifying entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matplotlib to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What? That's not a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so that's a scatterplot, which isn't much of a map. We have seen how Python allows us to easily combine different tools and modes of expression. We can use a package called `folium` to make OpenStreetMap / Google Maps style interactive maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, before we show a useful map, we need to decide how to centre it somewhere relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, the point associated with each row in geopandas is a `Shapely` geometric entity. It could also be a line, polygon, circle, or a combination of those, for example. We can also ask for the `gdf.geometry`, which is the collection of all the row entities.\n",
    "\n",
    "Shapely is a geometric package - this means we can do geometric operations on the rows. For example, we can combine all the points into one polka-dot geometric entity - a union operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.geometry.unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a 2d geometric entity, it also has a centroid - a weighted midpoint - which will suffice for us to centre our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle = gdf.geometry.unary_union.centroid\n",
    "middle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometric manipulation is much more useful than mapping - it is critical in many forms of numerical analysis, engineering, design and 3d printing. Explore Shapely's toolkit at your leisure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "def add(wreck):\n",
    "    pt = wreck.geometry\n",
    "    folium.Marker(\n",
    "        [pt.y, pt.x],\n",
    "        popup=wreck['Name'],\n",
    "    ).add_to(fmap)\n",
    "    return wreck\n",
    "\n",
    "gdf = gdf.apply(add, axis=1)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be familiar - as previously, we use the `apply` method on a dataframe to iterate through it (a geopandas geodataframe, but it's essentially the same idea). In each step, we add a marker to the map.\n",
    "\n",
    "In the first line, we create a map, setting the centre and zoom. For each wreck, we add its coordinates to a new folium `Marker` and call the `add_to` method to append it to the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you write a function that will take a string entry (it will be the content of the Date_Lost column) and return a string representing the century - '18', '19' or '20' - or None if it is not one of those? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLUE: `dateutil` has a parser that can make a best guess at which date a string represents: `dateutil.parser.parse(dt)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "\n",
    "def get_century(dt):\n",
    "    date = None\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add a `century` column to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add(wreck):\n",
    "    wreck['century'] = get_century(wreck['Date_lost'])\n",
    "    return wreck\n",
    "\n",
    "gdf = gdf.apply(add, axis=1)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many points don't have a century, but lets ignore that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folium also lets us group markers. This is much more useful. Here we create several `FeatureGroup` map layers and them to the map. We then add markers directly to each century's FeatureGroup with coloured icons. At the end, we add a layer control, so you can turn each group on and off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from folium import FeatureGroup, LayerControl\n",
    "\n",
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "centuries = {\n",
    "    '18': 'red',\n",
    "    '19': 'green',\n",
    "    '20': 'blue'\n",
    "}\n",
    "clusters = {}\n",
    "\n",
    "for century, colour in centuries.items():\n",
    "    clusters[century] = FeatureGroup(name=century)\n",
    "    clusters[century].add_to(fmap)\n",
    "\n",
    "def add(wreck):\n",
    "    wreck['century'] = get_century(wreck['Date_lost'])\n",
    "    \n",
    "    pt = wreck.geometry\n",
    "    \n",
    "    if wreck['century'] in centuries:\n",
    "        colour = centuries[wreck['century']]\n",
    "        folium.Marker(\n",
    "            [pt.y, pt.x],\n",
    "            popup=wreck['Name'],\n",
    "            icon=folium.Icon(color=colour)\n",
    "        ).add_to(clusters[wreck['century']])\n",
    "        \n",
    "    return wreck\n",
    "\n",
    "gdf = gdf.apply(add, axis=1)\n",
    "\n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing of the Seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a century. 1900's, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "century = '19' # if you want to try a different century, come back here, change this and run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kmeans isn't the best option, as lat-long is not linear and we are guessing cluster numbers,\n",
    "# but this illustrates how it can work\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "gdf['label'] = -1\n",
    "kdf = gdf[gdf['century'] == century]\n",
    "locations = np.array([[w.geometry.y, w.geometry.x] for n, w in kdf.iterrows()])\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our k-means algorithm - this involves choosing an expected number of clusters - and fit it to our locations list. It generates a numpy array of labels, one entry for each location representing its cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6).fit(locations)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then add these in the specific entries in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, label in enumerate(kmeans.labels_):\n",
    "    gdf.loc[kdf.index[n], 'label'] = label\n",
    "\n",
    "kdf = gdf[gdf['century'] == century]\n",
    "kdf.loc[:, 'label'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this look on the original map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "colours = ['red', 'green', 'blue', 'orange', 'pink', 'gray']\n",
    "clusters = {}\n",
    "\n",
    "for label, colour in enumerate(colours):\n",
    "    clusters[label] = FeatureGroup(name=\"Cluster %d\" % label)\n",
    "    clusters[label].add_to(fmap)\n",
    "\n",
    "def add(wreck):  \n",
    "    pt = wreck.geometry\n",
    "    \n",
    "    colour = colours[wreck['label']]\n",
    "    folium.Marker(\n",
    "        [pt.y, pt.x],\n",
    "        popup=wreck['Name'],\n",
    "        icon=folium.Icon(color=colour)\n",
    "    ).add_to(clusters[wreck['label']])\n",
    "    \n",
    "    return wreck\n",
    "\n",
    "kdf = kdf.apply(add, axis=1)\n",
    "\n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "* Which of NI's main harbours match to which clusters?\n",
    "* How does it change with the number of labels you use?\n",
    "* How does it change with the century?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question: is there a correlation between depth and number of shipwrecks?\n",
    "\n",
    "Rather than using the official bathymetry data, we will try and guess depth from shades of blue in satellite images. There's a few issues with our approach here - such as the resolution of the images, the processing that's already been done to them, the fact that colour isn't necessarily a good proxy for depth - but it will demonstrate how we can use image manipulation to back up other analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we show the Sentinel-2 open satellite data as a map layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "attr = \"\"\"\n",
    "<i>\n",
    "<a class=\"a-light\" xmlns:dct=\"http://purl.org/dc/terms/\"\n",
    "      href=\"https://s2maps.eu\" property=\"dct:title\">\n",
    "  Sentinel-2 cloudless - https://s2maps.eu\n",
    "</a> by\n",
    "<a class=\"a-light\" xmlns:cc=\"http://creativecommons.org/ns#\"\n",
    "      href=\"https://eox.at\" property=\"cc:attributionName\"\n",
    "      rel=\"cc:attributionURL\">\n",
    "  EOX IT Services GmbH\n",
    "</a><br/>\n",
    "(Contains modified Copernicus Sentinel data 2016 &amp; 2017)</i>\n",
    "\"\"\"\n",
    "sentinel = folium.WmsTileLayer(\n",
    "    url='https://tiles.maps.eox.at/wms',\n",
    "    name='s2maps.eu',\n",
    "    fmt='image/png',\n",
    "    attr=attr,\n",
    "    layers='s2cloudless_3857'\n",
    ").add_to(fmap)\n",
    "\n",
    "for cluster in clusters.values():\n",
    "    cluster.add_to(fmap)\n",
    "    \n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than examining _all_ this data - I have chopped out a tile for our convenience. An image we can manipulate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "tile = folium.raster_layers.ImageOverlay(\n",
    "    url='https://tiles.maps.eox.at/wms',\n",
    "    name='coast',\n",
    "    image='./data/coast.jpg',\n",
    "    fmt='jpg',\n",
    "    bounds=[[53.81683262680313, -6.78096405771856],[55.45928868149063, -5.14400116709356]],\n",
    "    attr=attr,\n",
    "    opacity=0.6,\n",
    "    zindex=1\n",
    ").add_to(fmap)\n",
    "\n",
    "for cluster in clusters.values():\n",
    "    cluster.add_to(fmap)\n",
    "    \n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we actually use a picture like data? scikit-image (skimage) has a lot of image manipulation tools - these can be coupled with scikit-learn in certain contexts to do image-based machine learning, so this is a brief intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coast = skimage.io.imread('./data/coast.jpg')\n",
    "plt.imshow(coast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have actually imported this image as a numpy array. In fact, this is actually plotting using the same technique as our Inflammation numpy tutorial - it's a graph of cells, where each cell has colour values. Pixels you might say..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coast[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we soften some of the noise using a Gaussian blur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "smooth = filters.gaussian(coast, sigma=10, multichannel=True)\n",
    "plt.imshow(smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a 3d matrix, as far as numpy is concerned, we can extract a single colour channel using a slice. Here is the green one (think RGB - G has index 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(smooth[:,:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, coarse way to trim all the non-sea (land, as it's known) is to pick everything that is more than a little green. Here we set anything with more than 0.2 in the green column to black."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "smooth[smooth[:,:,1] > 0.2] = [0, 0, 0]\n",
    "plt.imshow(smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we go one step further, by selecting _just_ the blue channel and trimming (setting to -1) everything that is outside of an emperical or heuristic (i.e. guessed or made up) window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sea = smooth[:,:,2].copy()\n",
    "sea[(sea < 0.1) | (sea > 0.25)] = -1\n",
    "plt.imshow(sea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we have some sea now - we know where it is. Now lets split it into layers of deep and shallow. To do this, we use a basic image segmentation technique, which is no more than numpy's `histogram` function. This puts all of our pixels into labeled \"bins\" (or layers, in our 2d image case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This number is our own choice of sea-depth layers\n",
    "bins = 5\n",
    "\n",
    "# We split up the sea into a set of bins\n",
    "hist, bin_edges = np.histogram(\n",
    "    sea,\n",
    "    bins=bins,\n",
    "    range=(sea[sea > 0].min(), sea.max())\n",
    ")\n",
    "\n",
    "# And create a new array called layers that matches each pixel to a depth layer: shallow...deep\n",
    "last_edge = bin_edges[0]\n",
    "layers = np.zeros(sea.shape, dtype=int) - 1\n",
    "for n, next_edge in enumerate(bin_edges[1:]):\n",
    "    layers[(sea > last_edge) & (sea <= next_edge)] = n\n",
    "    last_edge = next_edge\n",
    "plt.imshow(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the variety of layers - with as many colours as our `bins` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside for later - let's calculate what fraction of the visible sea each layer takes up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_frequencies = np.bincount(layers[layers >= 0].flatten())\n",
    "nonzero_pixels = sum(segment_frequencies)\n",
    "segment_fraction = segment_frequencies / nonzero_pixels\n",
    "segment_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to charts\n",
    "\n",
    "Does this match up with our map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "layer_bounds = [[53.81683262680313, -6.78096405771856],[55.45928868149063, -5.14400116709356]]\n",
    "\n",
    "tile = folium.raster_layers.ImageOverlay(\n",
    "    name='coast',\n",
    "    image=layers,\n",
    "    bounds=layer_bounds,\n",
    "    attr=attr,\n",
    "    opacity=0.6,\n",
    "    zindex=1\n",
    ").add_to(fmap)\n",
    "\n",
    "for cluster in clusters.values():\n",
    "    cluster.add_to(fmap)\n",
    "    \n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough, given all the approximations. Next...\n",
    "\n",
    "## Labelling our wrecks\n",
    "\n",
    "We need to create a function that we can pass a wreck to, and it will give its corresponding layer.\n",
    "\n",
    "We have a couple of convenient tools in scipy and numpy:\n",
    "* linspace : this creates an array linearly spread from one limit to another\n",
    "* meshgrid : this turns a set of horizontal and vertical axis ticks into a list of all the points in the corresponding grid\n",
    "* inter2p : this takes a series of axis ticks and function values at every grid point, and gives a callable function taking any coordinate whatsoever and returning the linearly-interpolated function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "layer_x = np.linspace(layer_bounds[0][1], layer_bounds[1][1], len(layers[0]))\n",
    "layer_y = np.linspace(layer_bounds[1][0], layer_bounds[0][0], len(layers))\n",
    "layers_flat = layers.copy().flatten()\n",
    "segment = scipy.interpolate.interp2d(layer_x, layer_y, layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this interpolated function to see what it looks like in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.set_zlim(-10, 10)\n",
    "ax.view_init(60, -90)\n",
    "\n",
    "X, Y = np.meshgrid(layer_x, layer_y)\n",
    "surf = ax.plot_surface(X, Y, segment(layer_x, layer_y), cmap=cm.coolwarm)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to use our interpolated labelling function, we turn back to our old dataframe method, `apply`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def point_to_segment(wreck):\n",
    "    interp = segment(wreck.geometry.x, wreck.geometry.y)\n",
    "    wreck['segment'] = int(round(interp[0]))\n",
    "    return wreck\n",
    "gdf = gdf.apply(point_to_segment, axis=1)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying this detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work out which segments we actually have wrecks in (we don't want to add more layers than necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments_active = set(gdf.loc[:, 'segment']) - {-1}\n",
    "segments_active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets create a copy of our dataframe for further manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldf = gdf.copy()\n",
    "ldf = gdf[gdf['century'] == '19'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much as above, we can run through our wrecks, but we use a handy colourmap library to give us shades indicating our depth layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "mid = min(segments_active) + (max(segments_active) - min(segments_active)) / 2\n",
    "colormap = cm.LinearColormap(\n",
    "    ['red', 'yellow', 'green'],\n",
    "    index=[min(segments_active), mid, max(segments_active)]\n",
    ")\n",
    "\n",
    "def add(wreck):\n",
    "    pt = wreck.geometry\n",
    "    \n",
    "    if wreck['segment'] > -1:\n",
    "        colour = colormap(wreck['segment'])\n",
    "        folium.CircleMarker(\n",
    "            [pt.y, pt.x],\n",
    "            popup=wreck['Name'],\n",
    "            fill_color=colour,\n",
    "            color=colour\n",
    "        ).add_to(fmap)\n",
    "        \n",
    "    return wreck\n",
    "\n",
    "ldf = ldf.apply(add, axis=1)\n",
    "\n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's nice, but it still doesn't answer our question - is there a correlation between depth and wreck frequency? Time to do some stats..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_table = ldf[ldf['segment'] > -1].groupby('segment').agg('count')\n",
    "\n",
    "total_segmented = segment_table['Name'].sum()\n",
    "\n",
    "def percentage(row):\n",
    "    row['Count'] = row['Name']\n",
    "    row['Blue Depth'] = bin_edges[row.name]\n",
    "    row['% wrecks'] = 100 * row['Name'] / total_segmented\n",
    "    row['% area'] = 100 * segment_fraction[row.name]\n",
    "    row['Wreck Frequency'] = row['% wrecks'] / row['% area']\n",
    "    return row\n",
    "\n",
    "segment_table = segment_table.apply(percentage, axis=1)\n",
    "segment_table = segment_table[['Count', 'Blue Depth', '% wrecks', '% area', 'Wreck Frequency']]\n",
    "segment_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So not hugely, but around a third higher for the shallow (mostly coastal) areas. Not a huge gap, but oh well... it may be worth trying for other centuries though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, let's put it all together and see what our shipwreck map now looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = folium.Map(location=(middle.y, middle.x), zoom_start=7)\n",
    "\n",
    "def add(wreck):\n",
    "    pt = wreck.geometry\n",
    "    \n",
    "    if wreck['segment'] > -1:\n",
    "        colour = colormap(wreck['segment'])\n",
    "        folium.CircleMarker(\n",
    "            [pt.y, pt.x],\n",
    "            popup=\"%s (%.2lf, %.2lf)\" % (wreck['Name'], wreck.geometry.y, wreck.geometry.x),\n",
    "            fill_color=colour,\n",
    "            fill_opacity=1,\n",
    "            radius=8,\n",
    "            color=colour\n",
    "        ).add_to(fmap)\n",
    "        \n",
    "    return wreck\n",
    "\n",
    "ldf.apply(add, axis=1)\n",
    "\n",
    "tile = folium.raster_layers.ImageOverlay(\n",
    "    name='coast',\n",
    "    image=layers,\n",
    "    bounds=layer_bounds,\n",
    "    attr=attr,\n",
    "    opacity=0.9,\n",
    "    zindex=1\n",
    ").add_to(fmap)\n",
    "\n",
    "LayerControl().add_to(fmap)\n",
    "fmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the above for different centuries. Do all centuries (with data) have similar results? Does this tell you something about seafaring over time? Are all century datasets equally valid for analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil\n",
    "\n",
    "def get_century(dt):\n",
    "    date = None\n",
    "    \n",
    "    if dt:\n",
    "        try:\n",
    "            date = dateutil.parser.parse(dt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        else:\n",
    "            if date and date.year:\n",
    "                date = str(date.year)[0:2]\n",
    "            else:\n",
    "                date = None\n",
    "\n",
    "    return date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
